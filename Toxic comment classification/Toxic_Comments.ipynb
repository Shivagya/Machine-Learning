{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Toxic_Comments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxKXEDGvsuOn",
        "outputId": "64780555-b777-4f5e-c89d-5a70678b53c8"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2021-07-25T06:41:50.867946Z",
          "iopub.status.busy": "2021-07-25T06:41:50.867500Z",
          "iopub.status.idle": "2021-07-25T06:41:50.885975Z",
          "shell.execute_reply": "2021-07-25T06:41:50.884999Z",
          "shell.execute_reply.started": "2021-07-25T06:41:50.867857Z"
        },
        "id": "08ifkcomZLtE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch import cuda\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('max_colwidth', 100)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:41:55.729334Z",
          "iopub.status.busy": "2021-07-25T06:41:55.729018Z",
          "iopub.status.idle": "2021-07-25T06:41:55.755218Z",
          "shell.execute_reply": "2021-07-25T06:41:55.753944Z",
          "shell.execute_reply.started": "2021-07-25T06:41:55.729306Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrOivM7zZLtG",
        "outputId": "6660a25e-78c9-47fa-e3ca-17e440accf9a"
      },
      "source": [
        "# To use GPU if available, else CPU\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:41:56.854618Z",
          "iopub.status.busy": "2021-07-25T06:41:56.854283Z",
          "iopub.status.idle": "2021-07-25T06:41:59.257621Z",
          "shell.execute_reply": "2021-07-25T06:41:59.256757Z",
          "shell.execute_reply.started": "2021-07-25T06:41:56.854587Z"
        },
        "id": "zzeivIcFZLtI"
      },
      "source": [
        "# Reading train and test data sets\n",
        "train = pd.read_csv(\"./train.csv\")\n",
        "test = pd.read_csv(\"./test.csv\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihsGM977lLxe"
      },
      "source": [
        "# train = train.sample(5000)\n",
        "# test = test.sample(2000)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:41:59.444597Z",
          "iopub.status.busy": "2021-07-25T06:41:59.444262Z",
          "iopub.status.idle": "2021-07-25T06:41:59.452769Z",
          "shell.execute_reply": "2021-07-25T06:41:59.451673Z",
          "shell.execute_reply.started": "2021-07-25T06:41:59.444566Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTD9Mov8ZLtJ",
        "outputId": "55c75c14-3fe3-4e71-9fa6-48c3a98af2ce"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(159571, 8)\n",
            "(153164, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwrZh1jGBK12",
        "outputId": "63a708d6-13b3-46b9-c736-15c35263afe7"
      },
      "source": [
        "print(train.isna().sum())\n",
        "print(test.isna().sum())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id               0\n",
            "comment_text     0\n",
            "toxic            0\n",
            "severe_toxic     0\n",
            "obscene          0\n",
            "threat           0\n",
            "insult           0\n",
            "identity_hate    0\n",
            "dtype: int64\n",
            "id              0\n",
            "comment_text    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:42:00.944275Z",
          "iopub.status.busy": "2021-07-25T06:42:00.943918Z",
          "iopub.status.idle": "2021-07-25T06:42:00.964201Z",
          "shell.execute_reply": "2021-07-25T06:42:00.963410Z",
          "shell.execute_reply.started": "2021-07-25T06:42:00.944246Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "QmPhpl-fZLtK",
        "outputId": "bd2812bc-3e74-4809-b1a7-3f868c89055e"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They wer...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, Januar...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relev...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                                                                         comment_text  toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
              "0  0000997932d777bf  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They wer...      0             0        0       0       0              0\n",
              "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, Januar...      0             0        0       0       0              0\n",
              "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relev...      0             0        0       0       0              0\n",
              "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics...      0             0        0       0       0              0\n",
              "4  0001d958c54c6e35                                  You, sir, are my hero. Any chance you remember what page that's on?      0             0        0       0       0              0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:42:01.671866Z",
          "iopub.status.busy": "2021-07-25T06:42:01.671513Z",
          "iopub.status.idle": "2021-07-25T06:42:01.681854Z",
          "shell.execute_reply": "2021-07-25T06:42:01.680813Z",
          "shell.execute_reply.started": "2021-07-25T06:42:01.671833Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "kxRcEEd4ZLtK",
        "outputId": "94abbda0-ef44-4c20-a48c-223d80495fd0"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>== From RfC == \\n\\n The title is fine as it is, IMO.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>:If you have a look back at the source, the information I updated was the correct form. I can on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>I don't anonymously edit articles at all.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id                                                                                         comment_text\n",
              "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofu...\n",
              "1  0000247867823ef7                                                 == From RfC == \\n\\n The title is fine as it is, IMO.\n",
              "2  00013b17ad220c46                                           \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"\n",
              "3  00017563c3f7919a  :If you have a look back at the source, the information I updated was the correct form. I can on...\n",
              "4  00017695ad8997eb                                                            I don't anonymously edit articles at all."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:42:03.063775Z",
          "iopub.status.busy": "2021-07-25T06:42:03.063380Z",
          "iopub.status.idle": "2021-07-25T06:42:03.101041Z",
          "shell.execute_reply": "2021-07-25T06:42:03.098221Z",
          "shell.execute_reply.started": "2021-07-25T06:42:03.063743Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYcZTlz-ZLtL",
        "outputId": "b3734e7e-63b0-4530-89d1-5d882eeff7e4"
      },
      "source": [
        "for col_name in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n",
        "    print(\"For\", col_name, '\\n')\n",
        "    print(train[col_name].value_counts())\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For toxic \n",
            "\n",
            "0    144277\n",
            "1     15294\n",
            "Name: toxic, dtype: int64\n",
            "\n",
            "\n",
            "For severe_toxic \n",
            "\n",
            "0    157976\n",
            "1      1595\n",
            "Name: severe_toxic, dtype: int64\n",
            "\n",
            "\n",
            "For obscene \n",
            "\n",
            "0    151122\n",
            "1      8449\n",
            "Name: obscene, dtype: int64\n",
            "\n",
            "\n",
            "For threat \n",
            "\n",
            "0    159093\n",
            "1       478\n",
            "Name: threat, dtype: int64\n",
            "\n",
            "\n",
            "For insult \n",
            "\n",
            "0    151694\n",
            "1      7877\n",
            "Name: insult, dtype: int64\n",
            "\n",
            "\n",
            "For identity_hate \n",
            "\n",
            "0    158166\n",
            "1      1405\n",
            "Name: identity_hate, dtype: int64\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:42:27.837655Z",
          "iopub.status.busy": "2021-07-25T06:42:27.837297Z",
          "iopub.status.idle": "2021-07-25T06:42:28.253589Z",
          "shell.execute_reply": "2021-07-25T06:42:28.252589Z",
          "shell.execute_reply.started": "2021-07-25T06:42:27.837624Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "HMNPhxdUZLtM",
        "outputId": "04a2b137-e42c-49ff-d56b-2b621f8d4004"
      },
      "source": [
        "# Preparation of labels for multi-label classification\n",
        "train['list'] = train[train.columns[2:]].values.tolist()\n",
        "new_df = train[['comment_text', 'list']].copy()\n",
        "new_df.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They wer...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, Januar...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relev...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                          comment_text                list\n",
              "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They wer...  [0, 0, 0, 0, 0, 0]\n",
              "1  D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, Januar...  [0, 0, 0, 0, 0, 0]\n",
              "2  Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relev...  [0, 0, 0, 0, 0, 0]\n",
              "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics...  [0, 0, 0, 0, 0, 0]\n",
              "4                                  You, sir, are my hero. Any chance you remember what page that's on?  [0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:53:11.322846Z",
          "iopub.status.busy": "2021-07-25T06:53:11.322504Z",
          "iopub.status.idle": "2021-07-25T06:53:13.581164Z",
          "shell.execute_reply": "2021-07-25T06:53:13.580349Z",
          "shell.execute_reply.started": "2021-07-25T06:53:11.322814Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Tz6ayM0ZLtN",
        "outputId": "c71e12ef-0b31-4b7d-9c26-8d49cb8586d1"
      },
      "source": [
        "# Loading bert model from transformers\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=6,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:45:04.491164Z",
          "iopub.status.busy": "2021-07-25T06:45:04.490796Z",
          "iopub.status.idle": "2021-07-25T06:45:49.144682Z",
          "shell.execute_reply": "2021-07-25T06:45:49.143673Z",
          "shell.execute_reply.started": "2021-07-25T06:45:04.491133Z"
        },
        "id": "vRLwz_0_ZLtO"
      },
      "source": [
        "# Splitting train set into train and validation sets\n",
        "X = list(new_df[\"comment_text\"])\n",
        "y = list(new_df[\"list\"])\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=128)\n",
        "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=128)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:46:14.783444Z",
          "iopub.status.busy": "2021-07-25T06:46:14.783109Z",
          "iopub.status.idle": "2021-07-25T06:46:14.790055Z",
          "shell.execute_reply": "2021-07-25T06:46:14.788821Z",
          "shell.execute_reply.started": "2021-07-25T06:46:14.783412Z"
        },
        "id": "ECVA9hT6ZLtP"
      },
      "source": [
        "# Create torch dataset\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:46:47.234124Z",
          "iopub.status.busy": "2021-07-25T06:46:47.233767Z",
          "iopub.status.idle": "2021-07-25T06:46:47.238536Z",
          "shell.execute_reply": "2021-07-25T06:46:47.237497Z",
          "shell.execute_reply.started": "2021-07-25T06:46:47.234095Z"
        },
        "id": "PNvKdW5XZLtP"
      },
      "source": [
        "# Creating torch dataset for train and validation sets to pass as input to model\n",
        "train_dataset = Dataset(X_train_tokenized, y_train)\n",
        "val_dataset = Dataset(X_val_tokenized, y_val)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjsUtO7utcKN",
        "outputId": "c4f8c477-48eb-4c66-f35a-36ac837facc7"
      },
      "source": [
        "train_dataset[0]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'input_ids': tensor([  101, 13055, 26568,  2323,  6402,  1999, 11669, 13055, 26568,  2003,\n",
              "         11669,  1012,  1045,  5223, 13055, 26568,  1012,  1042,  1003,  1003,\n",
              "          1047,  2014,  2000,  3109,   999,  6390,  1012,  6356,  1012,  6146,\n",
              "          1012,  2871,   102,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'labels': tensor([1., 0., 0., 0., 0., 0.]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:48:57.452571Z",
          "iopub.status.busy": "2021-07-25T06:48:57.452199Z",
          "iopub.status.idle": "2021-07-25T06:48:57.458511Z",
          "shell.execute_reply": "2021-07-25T06:48:57.457487Z",
          "shell.execute_reply.started": "2021-07-25T06:48:57.452539Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "K88YmLMEZLtQ",
        "outputId": "bfb1e7b9-19bc-428a-ceb6-ac8c87838909"
      },
      "source": [
        "X_train[2]"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"\\n\\nThe Objectivity of this Discussion is doubtful (non-existent)\\n\\n(1) As indicated earlier, the section on Marxist leaders’ views is misleading:\\n\\n(a) it lays unwarranted and excessive emphasis on Trotsky, creating the misleading impression that other prominent Marxists (Marx, Engels, Lenin) did not advocate and/or practiced terrorism;\\n\\n(b) it lays unwarranted and excessive emphasis on the theoretical “rejection of individual terrorism”, creating the misleading impression that this is the main (only) Marxist position on terrorism. \\n\\n(2) The discussion is not being properly monitored:\\n\\n(a) no discernible attempt is being made to establish and maintain an acceptable degree of objectivity;\\n\\n(b) important and relevant scholarly works such as the International Encyclopedia of Terrorism are being ignored or illicitly excluded from the discussion;\\n\\n(c) though the only logical way to remedy the blatant imbalance in the above section is to include quotes by/on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with impunity by the apologists for Marxist terrorism who have done their best to sabotage and wreck both the article and the discussion.\\n\\n(3) Among the tactics deployed by the apologist wreckers and saboteurs the following may be identified as representative examples:\\n\\n(a) it is claimed that Marx and Engels did not advocate terrorism despite the fact that scholarly works like the International Encyclopedia of Terrorism show that they did, and Marx himself was known as “The Red Terror Doctor”;\\n\\n(b) it is claimed that Marx and Engels were not involved in terrorist activities despite the fact that numerous sources from The Neue Rheinische Zeitung to Isaiah Berlin and Francis Wheen state otherwise;\\n\\n(c) it is claimed that Lenin does not refer to terror in The Proletarian Revolution and the Renegade K. Kautsky and other works/statements despite the fact that Robert Service, IET, and other scholarly and reliable sources state that he does;\\n\\n(d) it is claimed that the Russian word ‘’strakh’’ does not mean “terror” when:\\n\\ni. the Oxford Russian Dictionary says that it does;\\n\\nii. it is evident from the context that this is the case;\\n\\niii. any educated Russian speaker can confirm that strakh may mean “terror” depending on the context;\\n\\n(e) it is claimed that Marxism is “scientific” when in fact:\\n\\ni. Marx was not a scientist;\\n\\nii. Marx’s background was philosophy and law, not science;\\n\\niii. Marxism is not recognized as a science by the academic world;\\n\\niv. virtually every one of Marx’s predictions turned out to be wrong, as became increasingly apparent during his lifetime and incontrovertibly so after his death (R. Pipes, Communism: A Brief History, 2001, p. 15) from which it follows that Marxism does not qualify as a scientific system by any accepted standards;\\n\\nv. the evidence indicates that Marxism is closer to a religious sect than to science proper;\\n\\n(f) apologist literature is being quoted in a fraudulent attempt to whitewash Marxist terrorism, in effect turning the discussion into an advertisement for terrorism;\\n\\n(g) it is claimed that Marxist terrorism is not rooted in the Marxist theory of class struggle even though there are numerous sources showing that it is (please note that it is immaterial whether terrorism had already been justified in terms of a theory of class prior to Marx, the point being that it was advocated/practiced on the basis of Marxist class-struggle theories BY MARXISTS):\\n\\n“Karl Marx felt that terror was a necessary part of a revolutionary strategy” (Peter Galvert, “Theories of Terror in Urban Insurrections”, IET, p. 138);\\n\\n“Revolutionary terrorism has its roots in a political ideology, from the Marxist-Leninist thinking of the Left, to the fascists found on the Right” (Noemi Gal-Or, \"\"Revolutionary Terrorism\"\", IET, p. 203);\\n\\n“… perhaps the most important key to Stalin’s motivation lies in the realm of ideology. The leitmotif of Soviet communist ideology in the 1920s and 1930s was class struggle – the inbuilt antagonism between mutually incompatible economic interest groups” (Geoffrey Robert, Stalins Wars, 2006, pp. 17-18);\\n\\nthis fact is supported not only by reliable academic sources, but by elementary logic: \\n\\n“In 1907 Mehring published in the magazine ‘’Neue Zeit (Vol. XXV 2, p. 164) extracts from a letter by Marx to Weydemeyer dated March 5, 1852. In this letter, among other things, is the following noteworthy observation: … class struggle necessarily leads to the dictatorship of the proletariat …”'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:47:36.549436Z",
          "iopub.status.busy": "2021-07-25T06:47:36.549008Z",
          "iopub.status.idle": "2021-07-25T06:47:36.556725Z",
          "shell.execute_reply": "2021-07-25T06:47:36.555798Z",
          "shell.execute_reply.started": "2021-07-25T06:47:36.549397Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "CVR_fk49ZLtR",
        "outputId": "a997f79d-7d5d-48b9-b89d-1ddb7bc8931d"
      },
      "source": [
        "# sanity check\n",
        "tokenizer.decode(train_dataset[2][\"input_ids\"])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] \" the objectivity of this discussion is doubtful ( non - existent ) ( 1 ) as indicated earlier, the section on marxist leaders ’ views is misleading : ( a ) it lays unwarranted and excessive emphasis on trotsky, creating the misleading impression that other prominent marxists ( marx, engels, lenin ) did not advocate and / or practiced terrorism ; ( b ) it lays unwarranted and excessive emphasis on the theoretical “ rejection of individual terrorism ”, creating the misleading impression that this is the main ( only ) marxist position on terrorism. ( 2 ) the discussion is not being properly monitored : ( a [SEP]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:53:52.926737Z",
          "iopub.status.busy": "2021-07-25T06:53:52.926359Z",
          "iopub.status.idle": "2021-07-25T06:53:52.932634Z",
          "shell.execute_reply": "2021-07-25T06:53:52.931427Z",
          "shell.execute_reply.started": "2021-07-25T06:53:52.926676Z"
        },
        "id": "kql_1hJrZLtR"
      },
      "source": [
        "# Overriding compute_loss function of trainer to use BCE loss, since this is a multilabel classification task\n",
        "class MultilabelTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n",
        "                        labels.float().view(-1, self.model.config.num_labels))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:54:21.373258Z",
          "iopub.status.busy": "2021-07-25T06:54:21.372937Z",
          "iopub.status.idle": "2021-07-25T06:54:21.378647Z",
          "shell.execute_reply": "2021-07-25T06:54:21.377807Z",
          "shell.execute_reply.started": "2021-07-25T06:54:21.373231Z"
        },
        "id": "iN4nP6g3ZLtR"
      },
      "source": [
        "# Function to calculate accuracy for multilabel classification\n",
        "def accuracy_thresh(y_pred, y_true, thresh=0.5, sigmoid=True): \n",
        "    y_pred = torch.from_numpy(y_pred)\n",
        "    y_true = torch.from_numpy(y_true)\n",
        "    if sigmoid: \n",
        "      y_pred = y_pred.sigmoid()\n",
        "    return ((y_pred>thresh)==y_true.bool()).float().mean().item()"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:59:16.487015Z",
          "iopub.status.busy": "2021-07-25T06:59:16.486663Z",
          "iopub.status.idle": "2021-07-25T06:59:16.491279Z",
          "shell.execute_reply": "2021-07-25T06:59:16.490152Z",
          "shell.execute_reply.started": "2021-07-25T06:59:16.486987Z"
        },
        "id": "nQvy7l07ZLtR"
      },
      "source": [
        "# Function to compute metrics during training Epochs\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return {'accuracy_thresh': accuracy_thresh(predictions, labels)}"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:58:27.129895Z",
          "iopub.status.busy": "2021-07-25T06:58:27.129523Z",
          "iopub.status.idle": "2021-07-25T06:58:27.148860Z",
          "shell.execute_reply": "2021-07-25T06:58:27.147977Z",
          "shell.execute_reply.started": "2021-07-25T06:58:27.129863Z"
        },
        "id": "m89Q_wQ3ZLtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9327f0e-37ee-47d3-a253-357fa9eb96fa"
      },
      "source": [
        "# configure logging so we see training loss\n",
        "BATCH_SIZE = 16\n",
        "logging_steps = len(train_dataset) // BATCH_SIZE\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"bert_base_toxic\",             # Path to output the model files\n",
        "    evaluation_strategy=\"epoch\",              # Evaluation to be done after each epoch \n",
        "    per_device_train_batch_size=BATCH_SIZE,   # Batch size for train set\n",
        "    per_device_eval_batch_size=BATCH_SIZE,    # Batch size for test set\n",
        "    num_train_epochs=1,                       # Number of epochs for model training\n",
        "    seed=RANDOM_SEED,                 \n",
        "    learning_rate=2e-5,\n",
        "    fp16=True,\n",
        "    weight_decay=0.001,\n",
        "    logging_steps=logging_steps\n",
        ")"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T06:59:18.914212Z",
          "iopub.status.busy": "2021-07-25T06:59:18.913884Z",
          "iopub.status.idle": "2021-07-25T06:59:24.946319Z",
          "shell.execute_reply": "2021-07-25T06:59:24.945337Z",
          "shell.execute_reply.started": "2021-07-25T06:59:18.914183Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUDuaebkZLtS",
        "outputId": "e46c776c-2e77-4234-cbb6-b5f6749bb272"
      },
      "source": [
        "# creating Trainer object\n",
        "trainer = MultilabelTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using amp fp16 backend\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-25T07:00:33.701890Z",
          "iopub.status.busy": "2021-07-25T07:00:33.701454Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dWFFz0nDZLtS",
        "outputId": "efbb3f8f-2901-481e-fe6f-df173d177709"
      },
      "source": [
        "# Train pre-trained model (Finetuning)\n",
        "trainer.train()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 127656\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 7979\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7979' max='7979' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7979/7979 27:57, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy Thresh</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.046900</td>\n",
              "      <td>0.037657</td>\n",
              "      <td>0.985122</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1310: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-500\n",
            "Configuration saved in bert_base_toxic/checkpoint-500/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-500/special_tokens_map.json\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1310: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-1000\n",
            "Configuration saved in bert_base_toxic/checkpoint-1000/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-1000/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-1500\n",
            "Configuration saved in bert_base_toxic/checkpoint-1500/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-1500/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-2000\n",
            "Configuration saved in bert_base_toxic/checkpoint-2000/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-2000/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-2500\n",
            "Configuration saved in bert_base_toxic/checkpoint-2500/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-2500/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-3000\n",
            "Configuration saved in bert_base_toxic/checkpoint-3000/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-3000/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-3500\n",
            "Configuration saved in bert_base_toxic/checkpoint-3500/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-3500/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-4000\n",
            "Configuration saved in bert_base_toxic/checkpoint-4000/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-4000/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-4500\n",
            "Configuration saved in bert_base_toxic/checkpoint-4500/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-4500/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-5000\n",
            "Configuration saved in bert_base_toxic/checkpoint-5000/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-5000/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-5500\n",
            "Configuration saved in bert_base_toxic/checkpoint-5500/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-5500/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-6000\n",
            "Configuration saved in bert_base_toxic/checkpoint-6000/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-6000/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-6500\n",
            "Configuration saved in bert_base_toxic/checkpoint-6500/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-6500/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-7000\n",
            "Configuration saved in bert_base_toxic/checkpoint-7000/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-7000/special_tokens_map.json\n",
            "Saving model checkpoint to bert_base_toxic/checkpoint-7500\n",
            "Configuration saved in bert_base_toxic/checkpoint-7500/config.json\n",
            "Model weights saved in bert_base_toxic/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in bert_base_toxic/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in bert_base_toxic/checkpoint-7500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 31915\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7979, training_loss=0.04693829012722004, metrics={'train_runtime': 1677.5327, 'train_samples_per_second': 76.097, 'train_steps_per_second': 4.756, 'total_flos': 8397227791208448.0, 'train_loss': 0.04693829012722004, 'epoch': 1.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxim-AgCZLtS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "12379509-db78-4421-df89-d076f27563e3"
      },
      "source": [
        "# sanity check that we can run evaluation\n",
        "trainer.evaluate()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 31915\n",
            "  Batch size = 16\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1995' max='1995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1995/1995 04:19]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'epoch': 1.0,\n",
              " 'eval_accuracy_thresh': 0.9851219654083252,\n",
              " 'eval_loss': 0.03765677288174629,\n",
              " 'eval_runtime': 259.9185,\n",
              " 'eval_samples_per_second': 122.789,\n",
              " 'eval_steps_per_second': 7.675}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU2AvyqfZLtT"
      },
      "source": [
        "### Testing model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emBEPvPxZLtT"
      },
      "source": [
        "# Tokenizing test data\n",
        "X_test = list(test[\"comment_text\"])\n",
        "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=128)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvsbtPoqZLtU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "378bc487-3c53-4b3d-ed77-6c4189118815"
      },
      "source": [
        "# Creating torch dataset for test data\n",
        "test_dataset = Dataset(X_test_tokenized)\n",
        "# Performing predictions for test data using trained model\n",
        "raw_pred, _, _ = trainer.predict(test_dataset)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Prediction *****\n",
            "  Num examples = 153164\n",
            "  Batch size = 16\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11568' max='1995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1995/1995 12:56]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohfPnmEPZLtU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db2d4b6-dd42-42b6-9234-8feb51bfb85e"
      },
      "source": [
        "raw_pred"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5.836 ,  0.3877,  4.32  , -1.104 ,  3.178 ,  0.6357],\n",
              "       [-6.977 , -7.85  , -7.582 , -7.973 , -7.574 , -7.83  ],\n",
              "       [-6.81  , -7.914 , -7.473 , -8.12  , -7.613 , -7.934 ],\n",
              "       ...,\n",
              "       [-6.977 , -7.85  , -7.535 , -7.977 , -7.61  , -7.85  ],\n",
              "       [-6.816 , -7.83  , -7.574 , -7.96  , -7.613 , -7.668 ],\n",
              "       [ 2.5   , -3.924 ,  1.187 , -5.605 , -0.3777, -4.734 ]],\n",
              "      dtype=float16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEMI6yn8ZLtV"
      },
      "source": [
        "# Sigmoid function to calculate probablities of predictions from trainer\n",
        "def sigmoid(X):\n",
        "   return 1/(1+np.exp(-X))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ96BEV8ZLtV"
      },
      "source": [
        "# Calculating probabilities \n",
        "probabilities = sigmoid(raw_pred)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p3wg6UhZLtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba1ff84e-f954-4fe8-cfe8-d897ca7677a3"
      },
      "source": [
        "probabilities"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.971e-01, 5.957e-01, 9.863e-01, 2.490e-01, 9.595e-01, 6.538e-01],\n",
              "       [9.327e-04, 3.889e-04, 5.093e-04, 3.448e-04, 5.136e-04, 3.982e-04],\n",
              "       [1.103e-03, 3.655e-04, 5.684e-04, 2.983e-04, 4.935e-04, 3.581e-04],\n",
              "       ...,\n",
              "       [9.327e-04, 3.889e-04, 5.336e-04, 3.433e-04, 4.954e-04, 3.889e-04],\n",
              "       [1.095e-03, 3.982e-04, 5.136e-04, 3.486e-04, 4.935e-04, 4.673e-04],\n",
              "       [9.243e-01, 1.938e-02, 7.666e-01, 3.662e-03, 4.067e-01, 8.713e-03]],\n",
              "      dtype=float16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbefEkPOZLtW"
      },
      "source": [
        "raw_pred_df = pd.DataFrame(probabilities)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8nrB9Z_ZLtW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "851cd803-9694-480b-baf4-fc1a1348cdfe"
      },
      "source": [
        "raw_pred_df"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.997070</td>\n",
              "      <td>0.595703</td>\n",
              "      <td>0.986328</td>\n",
              "      <td>0.249023</td>\n",
              "      <td>0.959473</td>\n",
              "      <td>0.653809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000933</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.000568</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.000411</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.000325</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153159</th>\n",
              "      <td>0.691895</td>\n",
              "      <td>0.003662</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>0.001484</td>\n",
              "      <td>0.127563</td>\n",
              "      <td>0.003325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153160</th>\n",
              "      <td>0.004719</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.000278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153161</th>\n",
              "      <td>0.000933</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.000389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153162</th>\n",
              "      <td>0.001095</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153163</th>\n",
              "      <td>0.924316</td>\n",
              "      <td>0.019379</td>\n",
              "      <td>0.766602</td>\n",
              "      <td>0.003662</td>\n",
              "      <td>0.406738</td>\n",
              "      <td>0.008713</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153164 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0         1         2         3         4         5\n",
              "0       0.997070  0.595703  0.986328  0.249023  0.959473  0.653809\n",
              "1       0.000933  0.000389  0.000509  0.000345  0.000514  0.000398\n",
              "2       0.001103  0.000365  0.000568  0.000298  0.000494  0.000358\n",
              "3       0.000863  0.000411  0.000503  0.000377  0.000514  0.000429\n",
              "4       0.001400  0.000325  0.000527  0.000289  0.000525  0.000335\n",
              "...          ...       ...       ...       ...       ...       ...\n",
              "153159  0.691895  0.003662  0.254883  0.001484  0.127563  0.003325\n",
              "153160  0.004719  0.000267  0.001012  0.000245  0.000863  0.000278\n",
              "153161  0.000933  0.000389  0.000534  0.000343  0.000495  0.000389\n",
              "153162  0.001095  0.000398  0.000514  0.000349  0.000494  0.000467\n",
              "153163  0.924316  0.019379  0.766602  0.003662  0.406738  0.008713\n",
              "\n",
              "[153164 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWyp4V_JZLtW"
      },
      "source": [
        "test_with_pred_df = test.join(raw_pred_df)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Ibj4oIZLtW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "810a4dab-996b-4f6c-9db7-89621711a8fa"
      },
      "source": [
        "test_with_pred_df"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofu...</td>\n",
              "      <td>0.997070</td>\n",
              "      <td>0.595703</td>\n",
              "      <td>0.986328</td>\n",
              "      <td>0.249023</td>\n",
              "      <td>0.959473</td>\n",
              "      <td>0.653809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>== From RfC == \\n\\n The title is fine as it is, IMO.</td>\n",
              "      <td>0.000933</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000509</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"</td>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.000568</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>:If you have a look back at the source, the information I updated was the correct form. I can on...</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.000411</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>I don't anonymously edit articles at all.</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.000325</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>0.000335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153159</th>\n",
              "      <td>fffcd0960ee309b5</td>\n",
              "      <td>. \\n i totally agree, this stuff is nothing but too-long-crap</td>\n",
              "      <td>0.691895</td>\n",
              "      <td>0.003662</td>\n",
              "      <td>0.254883</td>\n",
              "      <td>0.001484</td>\n",
              "      <td>0.127563</td>\n",
              "      <td>0.003325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153160</th>\n",
              "      <td>fffd7a9a6eb32c16</td>\n",
              "      <td>== Throw from out field to home plate. == \\n\\n Does it get there faster by throwing to cut off m...</td>\n",
              "      <td>0.004719</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.000278</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153161</th>\n",
              "      <td>fffda9e8d6fafa9e</td>\n",
              "      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I see your changes and agree this is \"\"more correct....</td>\n",
              "      <td>0.000933</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.000389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153162</th>\n",
              "      <td>fffe8f1340a79fc2</td>\n",
              "      <td>\" \\n\\n == \"\"One of the founding nations of the EU - Germany - has a Law of Return quite similar ...</td>\n",
              "      <td>0.001095</td>\n",
              "      <td>0.000398</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000349</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153163</th>\n",
              "      <td>ffffce3fb183ee80</td>\n",
              "      <td>\" \\n :::Stop already. Your bullshit is not welcome here. I'm no fool, and if you think that kind...</td>\n",
              "      <td>0.924316</td>\n",
              "      <td>0.019379</td>\n",
              "      <td>0.766602</td>\n",
              "      <td>0.003662</td>\n",
              "      <td>0.406738</td>\n",
              "      <td>0.008713</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>153164 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id                                                                                         comment_text         0         1         2         3         4         5\n",
              "0       00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofu...  0.997070  0.595703  0.986328  0.249023  0.959473  0.653809\n",
              "1       0000247867823ef7                                                 == From RfC == \\n\\n The title is fine as it is, IMO.  0.000933  0.000389  0.000509  0.000345  0.000514  0.000398\n",
              "2       00013b17ad220c46                                           \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"  0.001103  0.000365  0.000568  0.000298  0.000494  0.000358\n",
              "3       00017563c3f7919a  :If you have a look back at the source, the information I updated was the correct form. I can on...  0.000863  0.000411  0.000503  0.000377  0.000514  0.000429\n",
              "4       00017695ad8997eb                                                            I don't anonymously edit articles at all.  0.001400  0.000325  0.000527  0.000289  0.000525  0.000335\n",
              "...                  ...                                                                                                  ...       ...       ...       ...       ...       ...       ...\n",
              "153159  fffcd0960ee309b5                                        . \\n i totally agree, this stuff is nothing but too-long-crap  0.691895  0.003662  0.254883  0.001484  0.127563  0.003325\n",
              "153160  fffd7a9a6eb32c16  == Throw from out field to home plate. == \\n\\n Does it get there faster by throwing to cut off m...  0.004719  0.000267  0.001012  0.000245  0.000863  0.000278\n",
              "153161  fffda9e8d6fafa9e  \" \\n\\n == Okinotorishima categories == \\n\\n I see your changes and agree this is \"\"more correct....  0.000933  0.000389  0.000534  0.000343  0.000495  0.000389\n",
              "153162  fffe8f1340a79fc2  \" \\n\\n == \"\"One of the founding nations of the EU - Germany - has a Law of Return quite similar ...  0.001095  0.000398  0.000514  0.000349  0.000494  0.000467\n",
              "153163  ffffce3fb183ee80  \" \\n :::Stop already. Your bullshit is not welcome here. I'm no fool, and if you think that kind...  0.924316  0.019379  0.766602  0.003662  0.406738  0.008713\n",
              "\n",
              "[153164 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_B7gZ9cZLtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60ccb75-836f-4722-df36-bfa64e90360e"
      },
      "source": [
        "test_with_pred_df.shape"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(153164, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dMxK9h6ZLtW"
      },
      "source": [
        "test_with_pred_df.to_csv(\"./test_with_prob_sigmoid_whole_train.csv\")"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdLrTeb4ZLtW"
      },
      "source": [
        ""
      ],
      "execution_count": 148,
      "outputs": []
    }
  ]
}